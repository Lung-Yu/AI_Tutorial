直觀的說，隨機森林可被視為是多個決策樹結合成的一個整體。 整體學習的背後想法是為了結合多個弱學習器來建構一個較強固的模型-強學習器! 一般來說，強學習器較不會發生高度過適問題，誤差也較低! 四步驟： 

1. 定義大小為n的隨機樣本數，採放回式(取出會放回)
2. 從自助樣本中導出決策樹，並對每一個節點隨機選擇d個特徵(取出不放回)，使用特徵分割該節點，依目標函數找出最佳方式。
3. 重複k次1-2步
4. 匯總，以多數決來指定類別標籤!


隨機森林的一個優點在於，我們不需要擔心如何選擇超參數值。 不需修剪隨機森林，因為強學習器不會因為雜訊影響，採多數決。 一般來說，k值愈大效果愈多，因為產生愈多的弱學習器，但效能是付出的學習成本。 n過大可能造成過適，而過小，也可以造成效能不佳! 多數的實作中，我們會採scikit-learn的預設置! 記得，隨機森林不需要做任何的資料預處理(標準化或是正規化、降維)

> https://www.facebook.com/notes/python-scikit-learn/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92_ml_randomforestclassifier%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97/802426066603094/
